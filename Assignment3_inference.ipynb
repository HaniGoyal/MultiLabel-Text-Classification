{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Multi-Label Text Classification Dataset.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "df['labels'] = df[df.columns[6:]].values.tolist()\n",
    "selected_columns = ['Title', 'abstractText', 'meshMajor', 'labels']\n",
    "df = df[selected_columns]\n",
    "\n",
    "#Training (70%), Testing (15%), Validation (15%)\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = train_data.reset_index(drop = True)\n",
    "test_data = test_data.reset_index(drop = True)\n",
    "val_data = val_data.reset_index(drop = True)\n",
    "\n",
    "features = ['Title', 'abstractText', 'meshMajor']\n",
    "targets_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
    "MAX_LEN = 512\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelArchitecture(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modelArchitecture, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', return_dict = True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer = nn.Linear(768, 14)\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = self.dropout(output.pooler_output)\n",
    "        output = self.layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelArchitecture(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (layer): Linear(in_features=768, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'model.pth'\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = modelArchitecture()\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(x, tokenizer, model):\n",
    "    wordList = []\n",
    "    wordList.append('Title:')\n",
    "    title_text = str(x['Title'])\n",
    "    wordList.extend(title_text.split())\n",
    "    wordList.append('Abstract:')\n",
    "    abstract_text = str(x['abstractText'])\n",
    "    wordList.extend(abstract_text.split())\n",
    "    wordList.append('Terms:')\n",
    "    mesh_text = str(x['meshMajor'])\n",
    "    wordList.extend(re.findall(r\"'(.*?)'\", mesh_text))\n",
    "\n",
    "    txt = \" \".join(wordList)\n",
    "\n",
    "    encodedText = tokenizer.encode_plus(\n",
    "        txt,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\",\n",
    "        return_token_type_ids=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    ids = encodedText['input_ids'].to(device)\n",
    "    mask = encodedText['attention_mask'].to(device)\n",
    "    token_type_ids = encodedText[\"token_type_ids\"].to(device)\n",
    "\n",
    "    output = model(ids, mask, token_type_ids)\n",
    "    output = torch.sigmoid(output).detach().cpu()\n",
    "    output = output.flatten().round().numpy().astype(int)\n",
    "    return list(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Title: Hydroxychloroquine in steroid dependent asthma.\n",
      "Abstract: A recent case report suggested that hydroxychloroquine had a steroid sparing effect in a patient with severe chronic asthma. We have studied the effect of hydroxychloroquine in a group of nine steroid dependent adult asthmatic patients using a randomised double blind crossover comparison of hydroxychloroquine and placebo. Each patient received hydroxychloroquine (400 mg/day) or placebo for 2 month periods. The effect of hydroxychloroquine or placebo on asthma control was assessed by change in steroid dosage, visual analogue symptom scores, response to beta 2 agonist and peak expiratory flow rate (PFR) measurement. The dose of prednisolone required during hydroxychloroquine treatment did not differ from that during placebo treatment or in pre-trial period. There was no significant change in symptom scores of PFR measurement. In this study an 8 week treatment with hydroxychloroquine was of no benefit to patients with chronic steroid dependent asthma.\n",
      "Meshmajor: ['Adult', 'Asthma', 'Beclomethasone', 'Double-Blind Method', 'Female', 'Humans', 'Hydroxychloroquine', 'Male', 'Middle Aged', 'Steroids']\n",
      "Predicted Labels: ['B', 'C', 'D', 'E', 'M', 'N']\n",
      "Actual Labels: ['B', 'C', 'D', 'E', 'M', 'N']\n",
      "2\n",
      "Title: Serotonergic modulation of odor input to the mammalian olfactory bulb.\n",
      "Abstract: Centrifugal serotonergic fibers innervate the olfactory bulb, but the importance of these projections for olfactory processing is unclear. We examined serotonergic modulation of sensory input to olfactory glomeruli using mice that express synaptopHluorin in olfactory receptor neurons (ORN). Odor-evoked synaptic input to glomeruli was attenuated by increased serotonin signaling through serotonin 2C (5-HT2C) receptors and amplified by decreased serotonergic activity. Intravital multiphoton calcium imaging revealed that 5-HT2C receptor activation amplified odor-evoked activity in a subset of juxtaglomerular cells and attenuated glutamate release from ORN terminals via GABA(B) receptors. Endogenous serotonin released by electrical stimulation of the dorsal raphe nucleus attenuated odor-evoked responses without detectable bias in glomerular position or odor identity. Weaker glomerular responses, however, were less sensitive to raphe stimulation than strong responses. Our data indicate that the serotonergic system regulates odor inputs in the olfactory bulb and suggest that behavioral states may alter odor processing at the earliest stages.\n",
      "Meshmajor: ['Animals', 'Calcium Signaling', 'Cell Differentiation', 'Efferent Pathways', 'Glutamic Acid', 'Mice', 'Mice, Transgenic', 'Nerve Tissue Proteins', 'Olfactory Bulb', 'Olfactory Pathways', 'Olfactory Receptor Neurons', 'Raphe Nuclei', 'Receptor, Serotonin, 5-HT2C', 'Receptors, GABA-B', 'Sensory Receptor Cells', 'Serotonin', 'Smell', 'Synapses', 'Synaptic Transmission']\n",
      "Predicted Labels: ['A', 'B', 'D', 'F', 'G']\n",
      "Actual Labels: ['A', 'B', 'D', 'F', 'G']\n",
      "3\n",
      "Title: Better glycaemic control with BioChaperone glargine lispro co-formulation than with insulin lispro Mix25 or separate glargine and lispro administrations after a test meal in people with type 2 diabetes.\n",
      "Abstract: Because of its physico-chemical properties, insulin glargine is usually not mixable with rapid insulins. BioChaperone BC147 is a polyanionic amphiphilic polymer, solubilizing insulin glargine at neutral pH, and thus enabling stable glargine formulation with fast-acting insulin lispro (BioChaperone glargine lispro co-formulation [BC Combo]). We investigated pharmacokinetic (PK) endpoints and postprandial glucose (PPG) control after administration of BC Combo (75% insulin glargine, 25% insulin lispro), insulin lispro Mix25 (LMix) and separate injections of insulins glargine (75% total dose) and lispro (25% total dose [G + L]) immediately before ingestion of a mixed meal in people with type 2 diabetes mellitus (T2DM), using a randomized, double-blind, double-dummy crossover study design. Participants received individualized bolus doses (mean 0.62 U/kg) of BC Combo, LMix or G + L, together with a solid mixed meal (610 kcal, 50% carbohydrate, 30% fat, 20% protein). Insulin dosages were kept constant for each study day. Thirty-nine participants with T2DM (mean ± SD age and glycated haemoglobin 60.8 ± 7.5 years and  64 ± 6 mmol/mol, respectively) were randomized. BC Combo improved the predefined primary endpoint, early PPG control, compared to LMix (incremental area under the blood glucose concentration-time curve from 0 to 2 hours after the meal [ÄAUCBG,0-2h ] reduction of 18%; P = 0.0009) and G + L (ÄAUCBG,0-2h reduction of 10%; P = 0.0450). The number of mealtime hypoglycaemic episodes per participant was lower with BC Combo (22 episodes in 14 participants) compared to LMix (43 episodes in 20 participants; P = 0.0028), but not significantly different from G + L (28 episodes in 19 participants; P = 0.2523). BC Combo demonstrated superior early PPG control with fewer hypoglycaemic episodes compared to LMix and superior early PPG control compared to separate G + L administrations.\n",
      "Meshmajor: ['Adolescent', 'Adult', 'Aged', 'Blood Glucose', 'Diabetes Mellitus, Type 2', 'Drug Combinations', 'Female', 'Humans', 'Hypoglycemia', 'Hypoglycemic Agents', 'Insulin Glargine', 'Insulin Lispro', 'Male', 'Middle Aged', 'Postprandial Period', 'Young Adult']\n",
      "Predicted Labels: ['B', 'C', 'D', 'E', 'G', 'J', 'M', 'N']\n",
      "Actual Labels: ['B', 'C', 'D', 'G', 'M']\n",
      "4\n",
      "Title: Signal detection by the PhoQ sensor-transmitter. Characterization of the sensor domain and a response-impaired mutant that identifies ligand-binding determinants.\n",
      "Abstract: The PhoP-PhoQ two-component system is required for virulence and/or regulatory stress responses in enteric bacteria. The PhoQ protein responds to low concentrations of extracellular divalent cations by activating PhoP-mediated transcription of a set of genes. PhoQ is a member of a family of transmembrane proteins that contain a periplasmic sensor domain coupled to a cytoplasmic transmitter domain. Here, we describe the cloning, purification, and properties of a fragment of Escherichia coli PhoQ corresponding to the sensor domain. This fragment is monomeric in solution and has a circular dichroism spectrum indicative of a mixture of alphahelix and beta-sheet. Divalent cations do not affect the oligomeric state, circular dichroism spectrum, or fluorescence spectrum of the sensor domain but do stabilize this domain to denaturation in a fashion expected for a direct binding model. We have also constructed a mutant in which a cluster of acidic amino acids (EDDDDAE) in the sensor domain is replaced with conservative, uncharged residues (QNNNNAQ). The mutant sensor domain is indistinguishable from wild type in terms of oligomeric form and spectral properties but differs in being substantially more stable to urea denaturation, showing no additional stabilization in the presence of divalent cations, and showing little activation of PhoP-mediated transcription in response to divalent-cation starvation in vivo. These data are consistent with a model in which divalent cations bind to the acidic cluster of the wild-type sensor domain and stabilize a conformation that is inactive in signaling. Substituting uncharged residues for the acidic cluster appears to mimic the effect of divalent-cation binding by stabilizing the inactive conformation.\n",
      "Meshmajor: ['Amino Acid Sequence', 'Bacterial Proteins', 'Binding Sites', 'Biosensing Techniques', 'Cations, Divalent', 'Molecular Sequence Data', 'Mutagenesis', 'Protein Denaturation', 'Protein Structure, Secondary', 'Tryptophan']\n",
      "Predicted Labels: ['D', 'E', 'G', 'L']\n",
      "Actual Labels: ['D', 'E', 'G', 'L']\n",
      "5\n",
      "Title: Treatment of Physiologic Gingival Pigmentation with Surgical Blade: A 25-Year Follow-up.\n",
      "Abstract: Gingival hyperpigmentation in the anterior region in people with a high smile line is an esthetic concern. Gingival depigmentation can be achieved through various procedures. Most techniques have shown successful short-term results; however, recurrence is observed in 50% of patients after 2 to 4 years. A 46-year-old woman presenting with a large black zone of gingival hyperpigmentation in the anterior maxilla was treated with scalpel blade excision. At 25 years after the depigmentation procedure, the patient displayed no recurrence.\n",
      "Meshmajor: ['Female', 'Follow-Up Studies', 'Gingiva', 'Gingival Diseases', 'Gingivectomy', 'Humans', 'Hyperpigmentation', 'Middle Aged', 'Time Factors']\n",
      "Predicted Labels: ['A', 'B', 'C', 'E', 'G', 'M', 'N']\n",
      "Actual Labels: ['A', 'B', 'C', 'E', 'G', 'M', 'N']\n"
     ]
    }
   ],
   "source": [
    "random_samples = test_data.sample(n = 5).reset_index(drop = True)\n",
    "\n",
    "for idx, row in random_samples.iterrows():\n",
    "    print(idx+1)\n",
    "    print(\"Title:\", row['Title'])\n",
    "    print(\"Abstract:\", row['abstractText'])\n",
    "    print(\"Meshmajor:\", row['meshMajor'])\n",
    "    predictions = infer(row[features], tokenizer, model)\n",
    "    labels = row['labels']\n",
    "    predicted_targets = [target for i, target in enumerate(targets_list) if predictions[i] == 1]\n",
    "    actual_targets = [target for i, target in enumerate(targets_list) if labels[i] == 1]\n",
    "    print(\"Predicted Labels:\", predicted_targets)\n",
    "    print(\"Actual Labels:\", actual_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
